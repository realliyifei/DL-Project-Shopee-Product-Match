{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"NN-Dense.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"H0ev4Npcs_tZ"},"source":["# Importing packages"]},{"cell_type":"code","metadata":{"id":"UjY7Yqhc3wCU"},"source":["!pip install torchviz"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NmPH0AhvU10J"},"source":["import pickle\n","import nltk\n","import pandas as pd\n","import numpy as np\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim\n","import matplotlib.pyplot as plt\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision import transforms, models\n","from torchsummary import summary\n","from torchviz import make_dot\n","from sklearn.metrics import log_loss, hamming_loss, accuracy_score, f1_score, roc_curve, auc\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","from gensim.models.doc2vec import Doc2Vec\n","from nltk.tokenize import word_tokenize\n","from PIL import Image\n","from tqdm import tqdm\n","nltk.download('punkt')\n","plt.rcParams['figure.figsize'] = (10, 8)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nytkr9zdQOB4"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n8HRb7MLtE8t"},"source":["# Copying the files to the local colab machine from google drive to speed up performance"]},{"cell_type":"code","metadata":{"id":"hxSfezlVgfAl"},"source":["!cp -r \"/content/drive/Shareddrives/CIS 522 Final Project/shopee-product-matching.zip\" .\n","!unzip \"/content/shopee-product-matching.zip\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pBFoFuCtukfz"},"source":["batch_size = 256"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tVUUj2CnvZ7s"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PSXz4C9MbK71"},"source":["pd.read_csv('/content/drive/Shareddrives/CIS 522 Final Project/Data/triplet_train.csv').head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_jJs1P7MtUHO"},"source":["# Defining image preprocessing and augmentation"]},{"cell_type":"code","metadata":{"id":"px_vjbzuQOsD"},"source":["train_transforms = transforms.Compose([\n","                                       transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n","                                       transforms.ColorJitter(),\n","                                       transforms.RandomHorizontalFlip(),\n","                                       transforms.Resize(size=256),\n","                                       transforms.CenterCrop(size=224),\n","                                       transforms.ToTensor(),\n","                                       transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","                                       ])\n","inference_transforms = transforms.Compose([\n","                                      transforms.Resize(size=256),\n","                                      transforms.CenterCrop(size=224),\n","                                      transforms.ToTensor(),\n","                                      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","                                      ])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mCAGLc3ItcfB"},"source":["# Defining the training dataset structure"]},{"cell_type":"code","metadata":{"id":"zCA0RPvXbyb1"},"source":["class ShopeeDatasetTrain(Dataset):\n","\n","    def __init__(self, csv_file, transform=None, folder='train'):\n","        \"\"\"\n","        Args:\n","            csv_file (string): Path to the csv file with annotations.\n","            root_dir (string): Directory with all the images.\n","            transform (callable, optional): Optional transform to be applied\n","                on a sample.\n","        \"\"\"\n","        self.dataframe = pd.read_csv(csv_file)\n","        self.nlp_model = Doc2Vec.load('/content/drive/Shareddrives/CIS 522 Final Project/Models/d2v.model')\n","        self.transform = transform\n","        self.folder = folder\n","\n","    def __len__(self):\n","        return len(self.dataframe)\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","        \n","        anchor_image = Image.open('{}_images/{}'.format(self.folder, self.dataframe.loc[idx, 'image_anchor']))\n","        positive_image = Image.open('{}_images/{}'.format(self.folder, self.dataframe.loc[idx, 'image_positive']))\n","        negative_image = Image.open('{}_images/{}'.format(self.folder, self.dataframe.loc[idx, 'image_negative']))\n","\n","        \n","        anchor_text = self.nlp_model.infer_vector(word_tokenize(self.dataframe.loc[idx, 'title_anchor'].lower()))\n","        positive_text = self.nlp_model.infer_vector(word_tokenize(self.dataframe.loc[idx, 'title_positive'].lower()))\n","        negative_text = self.nlp_model.infer_vector(word_tokenize(self.dataframe.loc[idx, 'title_negative'].lower()))\n","\n","        if self.transform:\n","            anchor_image = self.transform(anchor_image)\n","            positive_image = self.transform(positive_image)\n","            negative_image = self.transform(negative_image)\n","\n","        sample = anchor_image.cuda().float(), positive_image.cuda().float(), negative_image.cuda().float(), torch.from_numpy(anchor_text).cuda().float(), torch.from_numpy(positive_text).cuda().float(), torch.from_numpy(negative_text).cuda().float()\n","\n","        return sample"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"03U0C5mGtky8"},"source":["# Defining the inference dataset structure"]},{"cell_type":"code","metadata":{"id":"245gX0RB32pb"},"source":["class ShopeeDatasetInference(Dataset):\n","\n","    def __init__(self, csv_file, transform=None, folder='train'):\n","        \"\"\"\n","        Args:\n","            csv_file (string): Path to the csv file with annotations.\n","            root_dir (string): Directory with all the images.\n","            transform (callable, optional): Optional transform to be applied\n","                on a sample.\n","        \"\"\"\n","        self.dataframe = pd.read_csv(csv_file).drop_duplicates(subset=['posting_id_anchor'])\n","        self.nlp_model = Doc2Vec.load('/content/drive/Shareddrives/CIS 522 Final Project/Models/d2v.model')\n","        self.transform = transform\n","        self.folder = folder\n","\n","    def __len__(self):\n","        return len(self.dataframe)\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        \n","        image = Image.open('{}_images/{}'.format(self.folder, self.dataframe.loc[idx, 'image_anchor']))\n","        text = self.nlp_model.infer_vector(word_tokenize(self.dataframe.loc[idx, 'title_anchor'].lower()))\n","        label = self.dataframe.loc[idx, 'label_group_positive']\n","        \n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        sample = image.cuda().float(), torch.from_numpy(text).cuda().float(), label\n","\n","        return sample"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HzP5tOrrtr-4"},"source":["train_dataset = ShopeeDatasetTrain('/content/drive/Shareddrives/CIS 522 Final Project/Data/triplet_train.csv', transform=train_transforms, folder='train')\n","valid_dataset = ShopeeDatasetTrain('/content/drive/Shareddrives/CIS 522 Final Project/Data/triplet_valid.csv', transform=inference_transforms, folder='train')\n","inference_dataset = ShopeeDatasetInference('/content/drive/Shareddrives/CIS 522 Final Project/Data/triplet_valid.csv', transform=inference_transforms, folder='train')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iA77-ft_tn-2"},"source":["# Defining the dataloader to fetch batches from the dataset"]},{"cell_type":"code","metadata":{"id":"1uk1FH1gaeIJ"},"source":["train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n","inference_loader = DataLoader(inference_dataset, batch_size=batch_size, shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J-J66zCkt90I"},"source":["# Defining the Neural Network architecture"]},{"cell_type":"code","metadata":{"id":"okLWWzXbPdo7"},"source":["class FullNet(nn.Module):   \n","    def __init__(self):\n","        super(FullNet, self).__init__()\n","\n","        self.linear_layers = nn.Sequential(\n","            nn.Linear((150528+300), 1024),\n","            nn.ReLU(),\n","            nn.Dropout(p=0.5),\n","            nn.Linear(1024, 512),\n","            nn.ReLU(),\n","            nn.Dropout(p=0.5),\n","            nn.Linear(512, 300),\n","        )\n","\n","    def forward(self, image, text_embedding):\n","        image = image.view(image.shape[0], -1)\n","        output = self.linear_layers(torch.cat((image, text_embedding), axis=1))\n","\n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X3YuyHju29x0"},"source":["dnn = FullNet().cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bPbEZxwfuHx9"},"source":["# Defining the training loop"]},{"cell_type":"code","metadata":{"id":"PHaVeQ8281lt"},"source":["def train_model(model, train_loader, valid_loader):\n","\n","  criterion = nn.TripletMarginLoss()\n","  optimizer = torch.optim.Adam(model.parameters())\n","\n","  epoch_losses = []\n","  epoch_predictions = []\n","  epoch_actuals = []\n","\n","  for epoch in range(1, 20):\n","\n","    total_loss = 0\n","\n","    model.train()\n","\n","    for anchor_image, positive_image, negative_image, anchor_text, positive_text, negative_text in tqdm(train_loader):\n","      optimizer.zero_grad()\n","\n","      anchor_output = model(anchor_image, anchor_text)\n","      positive_output = model(positive_image, positive_text)\n","      negative_output = model(negative_image, negative_text)\n","\n","\n","      loss = criterion(anchor_output, positive_output, negative_output)\n","      total_loss += loss.item()\n","\n","      loss.backward()\n","      optimizer.step()\n","\n","    print()\n","    print('epoch: {}, train loss: {}'.format(epoch, total_loss/len(train_loader)))\n","\n","\n","    torch.save(model.state_dict(), '/content/drive/Shareddrives/CIS 522 Final Project/dnn_model_epoch_{}'.format(epoch))\n","\n","    model.eval()\n","\n","    total_loss = 0\n","\n","    actual = []\n","    predicted = []\n","\n","    for anchor_image, positive_image, negative_image, anchor_text, positive_text, negative_text in tqdm(valid_loader):\n","      with torch.no_grad():\n","        anchor_output = model(anchor_image, anchor_text)\n","        positive_output = model(positive_image, positive_text)\n","        negative_output = model(negative_image, negative_text)\n","\n","        loss = criterion(anchor_output, positive_output, negative_output)\n","        total_loss += loss.item()\n","      \n","      positive_predicted = torch.where(F.cosine_similarity(anchor_output, positive_output, dim=1) > 0.5, 1, 0).detach().cpu().numpy().tolist()\n","      negative_predicted = torch.where(F.cosine_similarity(anchor_output, negative_output, dim=1) > 0.5, 1, 0).detach().cpu().numpy().tolist()\n","\n","      predicted += positive_predicted + negative_predicted\n","      actual += [1] * len(anchor_output) + [0] * len(anchor_output)\n","\n","    print()\n","    print('epoch: {}, valid loss: {}'.format(epoch, total_loss/len(valid_loader)))\n","\n","    epoch_losses.append(total_loss)\n","    epoch_predictions.append(predicted)\n","    epoch_actuals.append(actual)\n","\n","    with open('/content/drive/Shareddrives/CIS 522 Final Project/dnn_valid_losses.npy', 'wb') as f:\n","      np.save(f, np.array(epoch_losses))\n","    with open('/content/drive/Shareddrives/CIS 522 Final Project/dnn_valid_actuals.npy', 'wb') as f:\n","      np.save(f, np.array(epoch_actuals))\n","    with open('/content/drive/Shareddrives/CIS 522 Final Project/dnn_valid_predictions.npy', 'wb') as f:\n","      np.save(f, np.array(epoch_predictions))\n","\n","  return epoch_losses, epoch_predictions, epoch_actuals"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G5xKIiV9uLVg"},"source":["# Defining the inference loop"]},{"cell_type":"code","metadata":{"id":"mOnLE_g96iUS"},"source":["def inference_model(model, inference_loader):\n","\n","  model.eval()\n","\n","  outputs = []\n","  labels = []\n","\n","  with torch.no_grad():\n","    for batch_images, batch_texts, batch_labels in tqdm(inference_loader):\n","      output = model(batch_images, batch_texts)\n","\n","      outputs += output.detach().cpu().numpy().tolist()\n","      labels += batch_labels.detach().cpu().numpy().tolist()\n","\n","\n","  return outputs, labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UuhGysgbc4lO"},"source":["epoch_losses, epoch_predictions, epoch_actuals = train_model(dnn, train_loader, valid_loader)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BSWKFon_uRCb"},"source":["# Computing and displaying the results for all the metrics (loss, accuracy, F1-micro, F1-macro)"]},{"cell_type":"code","metadata":{"id":"0tC6WIlBeUVZ"},"source":["epoch_losses = np.load('/content/drive/Shareddrives/CIS 522 Final Project/dnn_valid_losses.npy')\n","epoch_predictions = np.load('/content/drive/Shareddrives/CIS 522 Final Project/dnn_valid_predictions.npy')\n","epoch_actuals = np.load('/content/drive/Shareddrives/CIS 522 Final Project/dnn_valid_actuals.npy')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HnWCuP-p85Kx"},"source":["epoch_actuals.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M0iSdXkwq-TW"},"source":["fig = plt.figure()\n","plt.plot(epoch_losses)\n","plt.xlabel('epoch', fontsize=16)\n","plt.ylabel('Validation Triplet Loss', fontsize=16)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M-ORYR0YCm4X"},"source":["best_epoch = np.argmin(epoch_losses)\n","best_epoch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JhHs0wCspg2v"},"source":["fpr, tpr, thresholds = roc_curve(epoch_actuals[best_epoch].reshape(-1), epoch_predictions[best_epoch].reshape(-1))\n","roc_auc = auc(fpr, tpr)\n","\n","plt.title('Receiver Operating Characteristic')\n","plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n","plt.legend(loc = 'lower right')\n","plt.plot([0, 1], [0, 1],'r--')\n","plt.xlim([0, 1])\n","plt.ylim([0, 1])\n","plt.ylabel('True Positive Rate', fontsize=16)\n","plt.xlabel('False Positive Rate', fontsize=16)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LLH1HKSuE461"},"source":["points = [accuracy_score(epoch_actuals[i].reshape(-1), epoch_predictions[i].reshape(-1)) for i in range(len(epoch_actuals))]\n","\n","fig = plt.figure()\n","plt.plot(points)\n","plt.xlabel('epoch', fontsize=16)\n","plt.ylabel('Validation accuracy score', fontsize=16)\n","plt.ylim([0, 1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4u_tZjwP8RJi"},"source":["points = [f1_score(epoch_actuals[i].reshape(-1), epoch_predictions[i].reshape(-1)) for i in range(len(epoch_actuals))]\n","fig = plt.figure()\n","plt.plot(points)\n","plt.xlabel('epoch', fontsize=16)\n","plt.ylabel('Validation F1 score', fontsize=16)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xSV4Md8cqVmW"},"source":["all_outputs = []\n","all_labels = []\n","\n","for epoch in range(1, len(epoch_actuals)+1):\n","  dnn.load_state_dict(torch.load('/content/drive/Shareddrives/CIS 522 Final Project/dnn_model_epoch_{}'.format(epoch)))\n","  outputs, labels = inference_model(dnn, inference_loader)\n","\n","  all_outputs.append(outputs)\n","  all_labels.append(labels)\n","\n","  with open('/content/drive/Shareddrives/CIS 522 Final Project/dnn_valid_outputs.npy', 'wb') as f:\n","    np.save(f, np.array(all_outputs))\n","  with open('/content/drive/Shareddrives/CIS 522 Final Project/dnn_valid_labels.npy', 'wb') as f:\n","    np.save(f, np.array(all_labels))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zVO-51rogh53"},"source":["outputs = np.load('/content/drive/Shareddrives/CIS 522 Final Project/dnn_valid_outputs.npy')\n","labels = np.load('/content/drive/Shareddrives/CIS 522 Final Project/dnn_valid_labels.npy')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ADcbrVyXPXF6"},"source":["accuracies = []\n","f1_micros = []\n","f1_macros = []\n","\n","for epoch in range(1, len(epoch_actuals)):\n","  X_train, X_test, y_train, y_test = train_test_split(outputs[epoch], labels[epoch], test_size=0.20, random_state=1)\n","\n","  knn = KNeighborsClassifier(n_neighbors=1, n_jobs=-1)\n","  knn.fit(X_train, y_train)\n","\n","  results = knn.predict(X_test)\n","\n","  accuracies.append(accuracy_score(y_test, results))\n","  f1_micros.append(f1_score(y_test, results, average='micro'))\n","  f1_macros.append(f1_score(y_test, results, average='macro'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dqLUrE93RCFP"},"source":["fig = plt.figure()\n","plt.plot(accuracies)\n","plt.xlabel('epoch', fontsize=16)\n","plt.ylabel('Validation accuracy score', fontsize=16)\n","plt.ylim([0, 1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z1AgTcmQhC4k"},"source":["fig = plt.figure()\n","plt.plot(f1_micros)\n","plt.xlabel('epoch', fontsize=16)\n","plt.ylabel('Validation f1-micro score', fontsize=16)\n","plt.ylim([0, 1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o1FIPPSdhE_d"},"source":["fig = plt.figure()\n","plt.plot(f1_macros)\n","plt.xlabel('epoch', fontsize=16)\n","plt.ylabel('Validation f1-macro score', fontsize=16)\n","plt.ylim([0, 1])"],"execution_count":null,"outputs":[]}]}